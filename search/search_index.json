{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CORAL implementation for ordinal regression with deep neural networks. About CONDOR, short for CONDitionals for Ordinal Regression, is a method for ordinal regression with deep neural networks, which addresses the rank inconsistency issue of other ordinal regression frameworks. It is compatible with any state-of-the-art deep neural network architecture, requiring only modification of the output layer, the labels, the loss function. This repository implements the CONDOR functionality (neural network layer, loss function, and dataset utilities) for convenient use. We also have CONDOR implemented for Tensorflow . Docker We provide Dockerfile's to help get up and started quickly with condor_pytorch . The cpu image can be built and ran as follows, with tutorial jupyter notebooks built in. # Create a docker image, only done once docker build -t cpu_pytorch -f cpu.Dockerfile ./ # run image to serve a jupyter notebook docker run -it -p 8888:8888 --rm cpu_pytorch # how to run bash inside container (with python that will have deps) docker run -u $(id -u):$(id -g) -it -p 8888:8888 --rm cpu_pytorch bash An NVIDIA based gpu optimized container can be built and run as follows (without interactive ipynb capabilities). # only needs to be built once docker build -t gpu_pytorch -f gpu.Dockerfile ./ # use the image after building it docker run -it -p 8888:8888 --rm gpu_pytorch Cite as If you use CONDOR as part of your workflow in a scientific publication, please consider citing the CONDOR repository with the following DOI: Jenkinson, Khezeli, Oliver, Kalantari, Klee. Universally rank consistent ordinal regression in neural networks, arXiv:2110.07470, 2021. @article{condor2021, title = \"Universally rank consistent ordinal regression in neural networks\", journal = \"arXiv\", volume = \"2110.07470\", year = \"2021\", url = \"https://arxiv.org/abs/2110.07470\", author = \"Garrett Jenkinson and Kia Khezeli and Gavin R. Oliver and John Kalantari and Eric W. Klee\", keywords = \"Deep learning, Ordinal regression, neural networks, Machine learning, Biometrics\" }","title":"Home"},{"location":"#about","text":"CONDOR, short for CONDitionals for Ordinal Regression, is a method for ordinal regression with deep neural networks, which addresses the rank inconsistency issue of other ordinal regression frameworks. It is compatible with any state-of-the-art deep neural network architecture, requiring only modification of the output layer, the labels, the loss function. This repository implements the CONDOR functionality (neural network layer, loss function, and dataset utilities) for convenient use. We also have CONDOR implemented for Tensorflow .","title":"About"},{"location":"#docker","text":"We provide Dockerfile's to help get up and started quickly with condor_pytorch . The cpu image can be built and ran as follows, with tutorial jupyter notebooks built in. # Create a docker image, only done once docker build -t cpu_pytorch -f cpu.Dockerfile ./ # run image to serve a jupyter notebook docker run -it -p 8888:8888 --rm cpu_pytorch # how to run bash inside container (with python that will have deps) docker run -u $(id -u):$(id -g) -it -p 8888:8888 --rm cpu_pytorch bash An NVIDIA based gpu optimized container can be built and run as follows (without interactive ipynb capabilities). # only needs to be built once docker build -t gpu_pytorch -f gpu.Dockerfile ./ # use the image after building it docker run -it -p 8888:8888 --rm gpu_pytorch","title":"Docker"},{"location":"#cite-as","text":"If you use CONDOR as part of your workflow in a scientific publication, please consider citing the CONDOR repository with the following DOI: Jenkinson, Khezeli, Oliver, Kalantari, Klee. Universally rank consistent ordinal regression in neural networks, arXiv:2110.07470, 2021. @article{condor2021, title = \"Universally rank consistent ordinal regression in neural networks\", journal = \"arXiv\", volume = \"2110.07470\", year = \"2021\", url = \"https://arxiv.org/abs/2110.07470\", author = \"Garrett Jenkinson and Kia Khezeli and Gavin R. Oliver and John Kalantari and Eric W. Klee\", keywords = \"Deep learning, Ordinal regression, neural networks, Machine learning, Biometrics\" }","title":"Cite as"},{"location":"CHANGELOG/","text":"Release Notes The changelog for the current development version is available at https://github.com/GarrettJenkinson/condor_pytorch/blob/main/docs/CHANGELOG.md . 1.1.0 (02/19/2022) Downloads Source code (zip) Source code (tar.gz) New Features The condor_negloglikeloss function has been implemented to provide the maximume likelihood estimates of network parameters. Changes None. Bug Fixes None. 1.0.1 (11/01/2021) Downloads Source code (zip) Source code (tar.gz) New Features First release. Changes First release. Bug Fixes First release.","title":"Changelog"},{"location":"CHANGELOG/#release-notes","text":"The changelog for the current development version is available at https://github.com/GarrettJenkinson/condor_pytorch/blob/main/docs/CHANGELOG.md .","title":"Release Notes"},{"location":"CHANGELOG/#110-02192022","text":"","title":"1.1.0 (02/19/2022)"},{"location":"CHANGELOG/#downloads","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features","text":"The condor_negloglikeloss function has been implemented to provide the maximume likelihood estimates of network parameters.","title":"New Features"},{"location":"CHANGELOG/#changes","text":"None.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"None.","title":"Bug Fixes"},{"location":"CHANGELOG/#101-11012021","text":"","title":"1.0.1 (11/01/2021)"},{"location":"CHANGELOG/#downloads_1","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_1","text":"First release.","title":"New Features"},{"location":"CHANGELOG/#changes_1","text":"First release.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_1","text":"First release.","title":"Bug Fixes"},{"location":"citing/","text":"If you use CONDOR as part of your workflow in a scientific publication, please consider citing the CONDOR repository with the following DOI: Jenkinson, Khezeli, Oliver, Kalantari, Klee. Universally rank consistent ordinal regression in neural networks, arXiv:2110.07470, 2021. @article{condor2021, title = \"Universally rank consistent ordinal regression in neural networks\", journal = \"arXiv\", volume = \"2110.07470\", year = \"2021\", url = \"https://arxiv.org/abs/2110.07470\", author = \"Garrett Jenkinson and Kia Khezeli and Gavin R. Oliver and John Kalantari and Eric W. Klee\", keywords = \"Deep learning, Ordinal regression, neural networks, Machine learning, Biometrics\" } Acknowledgments : Many thanks to the CORAL ordinal authors and the CORAL pytorch authors whose repos provided a roadmap for this codebase.","title":"Citing"},{"location":"installation/","text":"Installing condor_pytorch Requirements Condor requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0 PyPI You can install the latest stable release of condor_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install condor-pytorch The dependencies can be pip installed also using the included requirements.txt : pip install -r requirements.txt Latest GitHub Source Code You want to try out the latest features before they go live on PyPI? Install the condor_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/GarrettJenkinson/condor_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install Docker If one does not wish to install things locally, running a docker container can make it simple to run Condor pytorch. We provide Dockerfile's to help get up and started quickly with condor_pytorch . The cpu image can be built and ran as follows, with tutorial jupyter notebooks built in. # Create a docker image, only done once docker build -t cpu_pytorch -f cpu.Dockerfile ./ # run image to serve a jupyter notebook docker run -it -p 8888:8888 --rm cpu_pytorch # how to run bash inside container (with python that will have deps) docker run -u $(id -u):$(id -g) -it -p 8888:8888 --rm cpu_pytorch bash An NVIDIA based gpu optimized container can be built and run as follows (without interactive ipynb capabilities). # only needs to be built once docker build -t gpu_pytorch -f gpu.Dockerfile ./ # use the image after building it docker run -it -p 8888:8888 --rm gpu_pytorch","title":"Installation"},{"location":"installation/#installing-condor_pytorch","text":"","title":"Installing condor_pytorch"},{"location":"installation/#requirements","text":"Condor requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0","title":"Requirements"},{"location":"installation/#pypi","text":"You can install the latest stable release of condor_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install condor-pytorch The dependencies can be pip installed also using the included requirements.txt : pip install -r requirements.txt","title":"PyPI"},{"location":"installation/#latest-github-source-code","text":"You want to try out the latest features before they go live on PyPI? Install the condor_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/GarrettJenkinson/condor_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Latest GitHub Source Code"},{"location":"installation/#docker","text":"If one does not wish to install things locally, running a docker container can make it simple to run Condor pytorch. We provide Dockerfile's to help get up and started quickly with condor_pytorch . The cpu image can be built and ran as follows, with tutorial jupyter notebooks built in. # Create a docker image, only done once docker build -t cpu_pytorch -f cpu.Dockerfile ./ # run image to serve a jupyter notebook docker run -it -p 8888:8888 --rm cpu_pytorch # how to run bash inside container (with python that will have deps) docker run -u $(id -u):$(id -g) -it -p 8888:8888 --rm cpu_pytorch bash An NVIDIA based gpu optimized container can be built and run as follows (without interactive ipynb capabilities). # only needs to be built once docker build -t gpu_pytorch -f gpu.Dockerfile ./ # use the image after building it docker run -it -p 8888:8888 --rm gpu_pytorch","title":"Docker"},{"location":"license/","text":"MIT License Copyright (c) 2021 Garrett Jenkinson Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright (c) 2021 Garrett Jenkinson Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"api_modules/condor_pytorch.activations/ordinal_softmax/","text":"ordinal_softmax ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]])","title":"condor_pytorch.activations"},{"location":"api_modules/condor_pytorch.activations/ordinal_softmax/#ordinal_softmax","text":"ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]])","title":"ordinal_softmax"},{"location":"api_modules/condor_pytorch.dataset/label_to_levels/","text":"label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"condor_pytorch.dataset"},{"location":"api_modules/condor_pytorch.dataset/label_to_levels/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_modules/condor_pytorch.dataset/levels_from_labelbatch/","text":"levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"condor_pytorch.dataset"},{"location":"api_modules/condor_pytorch.dataset/levels_from_labelbatch/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_modules/condor_pytorch.dataset/logits_to_label/","text":"logits_to_label logits_to_label(logits) Converts predicted logits from extended binary format to integer class labels Parameters logits : torch.tensor, shape(n_examples, n_labels-1) Torch tensor consisting of probabilities returned by ORCA model. Examples >>> # 3 training examples, 6 classes >>> logits = torch.tensor([[ 0.934, -0.861, 0.323, -0.492, -0.295], ... [-0.496, 0.485, 0.267, 0.124, -0.058], ... [ 0.985, 0.967, -0.920, 0.819, -0.506]]) >>> logits_to_label(logits) tensor([1, 0, 2])","title":"condor_pytorch.dataset"},{"location":"api_modules/condor_pytorch.dataset/logits_to_label/#logits_to_label","text":"logits_to_label(logits) Converts predicted logits from extended binary format to integer class labels Parameters logits : torch.tensor, shape(n_examples, n_labels-1) Torch tensor consisting of probabilities returned by ORCA model. Examples >>> # 3 training examples, 6 classes >>> logits = torch.tensor([[ 0.934, -0.861, 0.323, -0.492, -0.295], ... [-0.496, 0.485, 0.267, 0.124, -0.058], ... [ 0.985, 0.967, -0.920, 0.819, -0.506]]) >>> logits_to_label(logits) tensor([1, 0, 2])","title":"logits_to_label"},{"location":"api_modules/condor_pytorch.dataset/proba_to_label/","text":"proba_to_label proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"condor_pytorch.dataset"},{"location":"api_modules/condor_pytorch.dataset/proba_to_label/#proba_to_label","text":"proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"proba_to_label"},{"location":"api_modules/condor_pytorch.losses/CondorOrdinalCrossEntropy/","text":"CondorOrdinalCrossEntropy CondorOrdinalCrossEntropy(logits, levels, importance_weights=None, reduction='mean') computes the condor loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. levels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=none) optional weights for the different labels in levels. a tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as none. reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> CondorOrdinalCrossEntropy(logits, levels) tensor(0.8259)","title":"condor_pytorch.losses"},{"location":"api_modules/condor_pytorch.losses/CondorOrdinalCrossEntropy/#condorordinalcrossentropy","text":"CondorOrdinalCrossEntropy(logits, levels, importance_weights=None, reduction='mean') computes the condor loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. levels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=none) optional weights for the different labels in levels. a tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as none. reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> CondorOrdinalCrossEntropy(logits, levels) tensor(0.8259)","title":"CondorOrdinalCrossEntropy"},{"location":"api_modules/condor_pytorch.losses/condor_negloglikeloss/","text":"condor_negloglikeloss condor_negloglikeloss(logits, labels, reduction='mean') computes the negative log likelihood loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. labels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> labels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> condor_negloglikeloss(logits, labels) tensor(0.4936)","title":"Condor negloglikeloss"},{"location":"api_modules/condor_pytorch.losses/condor_negloglikeloss/#condor_negloglikeloss","text":"condor_negloglikeloss(logits, labels, reduction='mean') computes the negative log likelihood loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. labels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> labels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> condor_negloglikeloss(logits, labels) tensor(0.4936)","title":"condor_negloglikeloss"},{"location":"api_modules/condor_pytorch.metrics/earth_movers_distance/","text":"earth_movers_distance earth_movers_distance(logits, levels, device='cpu', reduction='mean') Computes the Earth Movers Distance Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> earth_movers_distance(logits, levels) tensor(0.6943)","title":"condor_pytorch.metrics"},{"location":"api_modules/condor_pytorch.metrics/earth_movers_distance/#earth_movers_distance","text":"earth_movers_distance(logits, levels, device='cpu', reduction='mean') Computes the Earth Movers Distance Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> earth_movers_distance(logits, levels) tensor(0.6943)","title":"earth_movers_distance"},{"location":"api_modules/condor_pytorch.metrics/mean_absolute_error/","text":"mean_absolute_error mean_absolute_error(logits, levels, reduction='mean') Computes the mean absolute error of ordinal predictions. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> mean_absolute_error(logits, levels) tensor(0.)","title":"condor_pytorch.metrics"},{"location":"api_modules/condor_pytorch.metrics/mean_absolute_error/#mean_absolute_error","text":"mean_absolute_error(logits, levels, reduction='mean') Computes the mean absolute error of ordinal predictions. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> mean_absolute_error(logits, levels) tensor(0.)","title":"mean_absolute_error"},{"location":"api_modules/condor_pytorch.metrics/ordinal_accuracy/","text":"ordinal_accuracy ordinal_accuracy(logits, levels, device='cpu', tolerance=0, reduction='mean') Computes the accuracy with a tolerance for ordinal error. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. tolerance : integer Allowed error in the ordinal ranks that will count as a correct prediction. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> ordinal_accuracy(logits, levels) tensor(1.)","title":"condor_pytorch.metrics"},{"location":"api_modules/condor_pytorch.metrics/ordinal_accuracy/#ordinal_accuracy","text":"ordinal_accuracy(logits, levels, device='cpu', tolerance=0, reduction='mean') Computes the accuracy with a tolerance for ordinal error. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. tolerance : integer Allowed error in the ordinal ranks that will count as a correct prediction. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> ordinal_accuracy(logits, levels) tensor(1.)","title":"ordinal_accuracy"},{"location":"api_modules/condor_pytorch.metrics/ordinal_softmax/","text":"ordinal_softmax ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]])","title":"condor_pytorch.metrics"},{"location":"api_modules/condor_pytorch.metrics/ordinal_softmax/#ordinal_softmax","text":"ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]])","title":"ordinal_softmax"},{"location":"api_subpackages/condor_pytorch.activations/","text":"condor_pytorch version: 1.0.0 ordinal_softmax ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]])","title":"condor_pytorch.activations"},{"location":"api_subpackages/condor_pytorch.activations/#ordinal_softmax","text":"ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]])","title":"ordinal_softmax"},{"location":"api_subpackages/condor_pytorch.dataset/","text":"condor_pytorch version: 1.0.0 logits_to_label logits_to_label(logits) Converts predicted logits from extended binary format to integer class labels Parameters logits : torch.tensor, shape(n_examples, n_labels-1) Torch tensor consisting of probabilities returned by ORCA model. Examples >>> # 3 training examples, 6 classes >>> logits = torch.tensor([[ 0.934, -0.861, 0.323, -0.492, -0.295], ... [-0.496, 0.485, 0.267, 0.124, -0.058], ... [ 0.985, 0.967, -0.920, 0.819, -0.506]]) >>> logits_to_label(logits) tensor([1, 0, 2]) label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.]) proba_to_label proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5]) levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"condor_pytorch.dataset"},{"location":"api_subpackages/condor_pytorch.dataset/#logits_to_label","text":"logits_to_label(logits) Converts predicted logits from extended binary format to integer class labels Parameters logits : torch.tensor, shape(n_examples, n_labels-1) Torch tensor consisting of probabilities returned by ORCA model. Examples >>> # 3 training examples, 6 classes >>> logits = torch.tensor([[ 0.934, -0.861, 0.323, -0.492, -0.295], ... [-0.496, 0.485, 0.267, 0.124, -0.058], ... [ 0.985, 0.967, -0.920, 0.819, -0.506]]) >>> logits_to_label(logits) tensor([1, 0, 2])","title":"logits_to_label"},{"location":"api_subpackages/condor_pytorch.dataset/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_subpackages/condor_pytorch.dataset/#proba_to_label","text":"proba_to_label(probas) Converts predicted probabilities from extended binary format to integer class labels Parameters probas : torch.tensor, shape(n_examples, n_labels) Torch tensor consisting of probabilities returned by CORAL model. Examples >>> # 3 training examples, 6 classes >>> probas = torch.tensor([[0.934, 0.861, 0.323, 0.492, 0.295], ... [0.496, 0.485, 0.267, 0.124, 0.058], ... [0.985, 0.967, 0.920, 0.819, 0.506]]) >>> proba_to_label(probas) tensor([2, 0, 5])","title":"proba_to_label"},{"location":"api_subpackages/condor_pytorch.dataset/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_subpackages/condor_pytorch.losses/","text":"condor_pytorch version: 1.0.0 condor_negloglikeloss condor_negloglikeloss(logits, labels, reduction='mean') computes the negative log likelihood loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. labels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> labels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> condor_negloglikeloss(logits, labels) tensor(0.4936) CondorOrdinalCrossEntropy CondorOrdinalCrossEntropy(logits, levels, importance_weights=None, reduction='mean') computes the condor loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. levels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=none) optional weights for the different labels in levels. a tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as none. reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> CondorOrdinalCrossEntropy(logits, levels) tensor(0.8259)","title":"condor_pytorch.losses"},{"location":"api_subpackages/condor_pytorch.losses/#condor_negloglikeloss","text":"condor_negloglikeloss(logits, labels, reduction='mean') computes the negative log likelihood loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. labels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> labels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> condor_negloglikeloss(logits, labels) tensor(0.4936)","title":"condor_negloglikeloss"},{"location":"api_subpackages/condor_pytorch.losses/#condorordinalcrossentropy","text":"CondorOrdinalCrossEntropy(logits, levels, importance_weights=None, reduction='mean') computes the condor loss described in condor tbd. parameters logits : torch.tensor, shape(num_examples, num_classes-1) outputs of the condor layer. levels : torch.tensor, shape(num_examples, num_classes-1) true labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=none) optional weights for the different labels in levels. a tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as none. reduction : str or none (default='mean') if 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. if none, returns a vector of shape (num_examples,) returns loss : torch.tensor a torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=none ). examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> CondorOrdinalCrossEntropy(logits, levels) tensor(0.8259)","title":"CondorOrdinalCrossEntropy"},{"location":"api_subpackages/condor_pytorch.metrics/","text":"condor_pytorch version: 1.0.0 ordinal_accuracy ordinal_accuracy(logits, levels, device='cpu', tolerance=0, reduction='mean') Computes the accuracy with a tolerance for ordinal error. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. tolerance : integer Allowed error in the ordinal ranks that will count as a correct prediction. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> ordinal_accuracy(logits, levels) tensor(1.) ordinal_softmax ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]]) mean_absolute_error mean_absolute_error(logits, levels, reduction='mean') Computes the mean absolute error of ordinal predictions. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> mean_absolute_error(logits, levels) tensor(0.) earth_movers_distance earth_movers_distance(logits, levels, device='cpu', reduction='mean') Computes the Earth Movers Distance Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> earth_movers_distance(logits, levels) tensor(0.6943)","title":"condor_pytorch.metrics"},{"location":"api_subpackages/condor_pytorch.metrics/#ordinal_accuracy","text":"ordinal_accuracy(logits, levels, device='cpu', tolerance=0, reduction='mean') Computes the accuracy with a tolerance for ordinal error. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. tolerance : integer Allowed error in the ordinal ranks that will count as a correct prediction. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> ordinal_accuracy(logits, levels) tensor(1.)","title":"ordinal_accuracy"},{"location":"api_subpackages/condor_pytorch.metrics/#ordinal_softmax","text":"ordinal_softmax(x, device='cpu') Convert the ordinal logit output to label probabilities. Parameters x: torch.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. Returns probs_tensor: torch.Tensor, shape=(num_samples, num_classes) Probabilities of each class (columns) for each sample (rows). Examples >>> ordinal_softmax(torch.tensor([[-1.,1],[-2,2]])) tensor([[0.7311, 0.0723, 0.1966], [0.8808, 0.0142, 0.1050]])","title":"ordinal_softmax"},{"location":"api_subpackages/condor_pytorch.metrics/#mean_absolute_error","text":"mean_absolute_error(logits, levels, reduction='mean') Computes the mean absolute error of ordinal predictions. Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> mean_absolute_error(logits, levels) tensor(0.)","title":"mean_absolute_error"},{"location":"api_subpackages/condor_pytorch.metrics/#earth_movers_distance","text":"earth_movers_distance(logits, levels, device='cpu', reduction='mean') Computes the Earth Movers Distance Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CONDOR layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via condor_pytorch.dataset.levels_from_labelbatch ). device: 'cpu', 'cuda', or None (default='cpu') If GPUs are utilized, then the device should be passed accordingly. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> earth_movers_distance(logits, levels) tensor(0.6943)","title":"earth_movers_distance"},{"location":"tutorials/mnist/","text":"CONDOR CNN for predicting handwritten digits (MNIST) This tutorial explains how to equip a deep neural network with the CONDOR layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CONDOR. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True) test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor()) train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True, shuffle=True) test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=True, shuffle=False) # Checking the dataset for images, labels in train_loader: print('Image batch dimensions:', images.shape) print('Image label dimensions:', labels.shape) break 0.1% Training on cpu Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz 100.0% Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw 102.8% Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz 19.9% Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz 100.0% Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz 112.7% Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw Processing... Done! Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) /Users/m191034/opt/anaconda3/envs/condor_pytorch/lib/python3.9/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /tmp/pip-req-build-5cal76n6/torch/csrc/utils/tensor_numpy.cpp:180.) return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s) 2 - Equipping CNN with CONDOR layer In this section, we are using condor_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CONDOR method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CONDOR layer. Using the Sequential API, we specify the CORAl layer as self.fc = torch.nn.Linear(size_in=294, num_classes=num_classes-1) This is because the convolutional and pooling layers torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) produce a flattened feature vector of 294 units. Then, when using the CONDOR layer in the forward function logits = self.fc(x) please use the sigmoid not softmax function (since the CONDOR method uses a concept known as extended binary classification as described in the paper). class ConvNet(torch.nn.Module): def __init__(self, num_classes): super(ConvNet, self).__init__() self.features = torch.nn.Sequential( torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) self.fc = torch.nn.Linear(294,num_classes-1) #THIS IS KEY OUTPUT SIZE def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) # flatten logits = self.fc(x) return logits torch.manual_seed(random_seed) model = ConvNet(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters()) 3 - Using the CONDOR loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via condor_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CONDOR loss (also provided via condor_pytorch ): cost = condor_negloglikeloss(logits, levels) from condor_pytorch.dataset import levels_from_labelbatch from condor_pytorch.losses import condor_negloglikeloss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits = model(features) #### CONDOR loss cost = cost = condor_negloglikeloss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) /Users/m191034/opt/anaconda3/envs/condor_pytorch/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-5cal76n6/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 000/468 | Cost: 3.9491 Epoch: 001/010 | Batch 200/468 | Cost: 0.7655 Epoch: 001/010 | Batch 400/468 | Cost: 0.4426 Epoch: 002/010 | Batch 000/468 | Cost: 0.5158 Epoch: 002/010 | Batch 200/468 | Cost: 0.3747 Epoch: 002/010 | Batch 400/468 | Cost: 0.3749 Epoch: 003/010 | Batch 000/468 | Cost: 0.3082 Epoch: 003/010 | Batch 200/468 | Cost: 0.2842 Epoch: 003/010 | Batch 400/468 | Cost: 0.2252 Epoch: 004/010 | Batch 000/468 | Cost: 0.1634 Epoch: 004/010 | Batch 200/468 | Cost: 0.2194 Epoch: 004/010 | Batch 400/468 | Cost: 0.2034 Epoch: 005/010 | Batch 000/468 | Cost: 0.2358 Epoch: 005/010 | Batch 200/468 | Cost: 0.2009 Epoch: 005/010 | Batch 400/468 | Cost: 0.0824 Epoch: 006/010 | Batch 000/468 | Cost: 0.1473 Epoch: 006/010 | Batch 200/468 | Cost: 0.1436 Epoch: 006/010 | Batch 400/468 | Cost: 0.2141 Epoch: 007/010 | Batch 000/468 | Cost: 0.3440 Epoch: 007/010 | Batch 200/468 | Cost: 0.1331 Epoch: 007/010 | Batch 400/468 | Cost: 0.1123 Epoch: 008/010 | Batch 000/468 | Cost: 0.1986 Epoch: 008/010 | Batch 200/468 | Cost: 0.1621 Epoch: 008/010 | Batch 400/468 | Cost: 0.1798 Epoch: 009/010 | Batch 000/468 | Cost: 0.1393 Epoch: 009/010 | Batch 200/468 | Cost: 0.2675 Epoch: 009/010 | Batch 400/468 | Cost: 0.1482 Epoch: 010/010 | Batch 000/468 | Cost: 0.1138 Epoch: 010/010 | Batch 200/468 | Cost: 0.1673 Epoch: 010/010 | Batch 400/468 | Cost: 0.1629 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the logits_to_label utility function from condor_pytorch to convert the probabilities back to the orginal label. from condor_pytorch.dataset import logits_to_label from condor_pytorch.activations import ordinal_softmax from condor_pytorch.metrics import earth_movers_distance from condor_pytorch.metrics import ordinal_accuracy from condor_pytorch.metrics import mean_absolute_error def compute_mae_and_acc(model, data_loader, device): with torch.no_grad(): emd, mae, acc, acc1, num_examples = 0., 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(targets, num_classes=NUM_CLASSES) features = features.to(device) levels = levels.to(device) targets = targets.float().to(device) ids = targets.long() logits = model(features) predicted_labels = logits_to_label(logits).float() predicted_probs = ordinal_softmax(logits).float() num_examples += targets.size(0) mae += mean_absolute_error(logits,levels,reduction='sum') acc += ordinal_accuracy(logits,levels,tolerance=0,reduction='sum') acc1 += ordinal_accuracy(logits,levels,tolerance=1,reduction='sum') emd += earth_movers_distance(logits,levels,reduction='sum') mae = mae / num_examples acc = acc / num_examples acc1 = acc1 / num_examples emd = emd / num_examples return mae, acc, acc1, emd train_mae, train_acc, train_acc1, train_emd = compute_mae_and_acc(model, train_loader, DEVICE) test_mae, test_acc, test_acc1, test_emd = compute_mae_and_acc(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Accuracy tolerance 0 (train/test): {train_acc:.2f} | {test_acc:.2f}') print(f'Accuracy tolerance 1 (train/test): {train_acc1:.2f} | {test_acc1:.2f}') print(f'Earth movers distance (train/test): {train_emd:.3f} | {test_emd:.3f}') Mean absolute error (train/test): 0.15 | 0.15 Accuracy tolerance 0 (train/test): 0.96 | 0.96 Accuracy tolerance 1 (train/test): 0.97 | 0.97 Earth movers distance (train/test): 0.251 | 0.247","title":"MNIST"},{"location":"tutorials/mnist/#condor-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to equip a deep neural network with the CONDOR layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CONDOR CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CONDOR. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True) test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor()) train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True, shuffle=True) test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=True, shuffle=False) # Checking the dataset for images, labels in train_loader: print('Image batch dimensions:', images.shape) print('Image label dimensions:', labels.shape) break 0.1% Training on cpu Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz 100.0% Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw 102.8% Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz 19.9% Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz 100.0% Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz 112.7% Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw Processing... Done! Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) /Users/m191034/opt/anaconda3/envs/condor_pytorch/lib/python3.9/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /tmp/pip-req-build-5cal76n6/torch/csrc/utils/tensor_numpy.cpp:180.) return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/mnist/#2-equipping-cnn-with-condor-layer","text":"In this section, we are using condor_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CONDOR method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CONDOR layer. Using the Sequential API, we specify the CORAl layer as self.fc = torch.nn.Linear(size_in=294, num_classes=num_classes-1) This is because the convolutional and pooling layers torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) produce a flattened feature vector of 294 units. Then, when using the CONDOR layer in the forward function logits = self.fc(x) please use the sigmoid not softmax function (since the CONDOR method uses a concept known as extended binary classification as described in the paper). class ConvNet(torch.nn.Module): def __init__(self, num_classes): super(ConvNet, self).__init__() self.features = torch.nn.Sequential( torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) self.fc = torch.nn.Linear(294,num_classes-1) #THIS IS KEY OUTPUT SIZE def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) # flatten logits = self.fc(x) return logits torch.manual_seed(random_seed) model = ConvNet(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters())","title":"2 - Equipping CNN with CONDOR layer"},{"location":"tutorials/mnist/#3-using-the-condor-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via condor_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CONDOR loss (also provided via condor_pytorch ): cost = condor_negloglikeloss(logits, levels) from condor_pytorch.dataset import levels_from_labelbatch from condor_pytorch.losses import condor_negloglikeloss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits = model(features) #### CONDOR loss cost = cost = condor_negloglikeloss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) /Users/m191034/opt/anaconda3/envs/condor_pytorch/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /tmp/pip-req-build-5cal76n6/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) Epoch: 001/010 | Batch 000/468 | Cost: 3.9491 Epoch: 001/010 | Batch 200/468 | Cost: 0.7655 Epoch: 001/010 | Batch 400/468 | Cost: 0.4426 Epoch: 002/010 | Batch 000/468 | Cost: 0.5158 Epoch: 002/010 | Batch 200/468 | Cost: 0.3747 Epoch: 002/010 | Batch 400/468 | Cost: 0.3749 Epoch: 003/010 | Batch 000/468 | Cost: 0.3082 Epoch: 003/010 | Batch 200/468 | Cost: 0.2842 Epoch: 003/010 | Batch 400/468 | Cost: 0.2252 Epoch: 004/010 | Batch 000/468 | Cost: 0.1634 Epoch: 004/010 | Batch 200/468 | Cost: 0.2194 Epoch: 004/010 | Batch 400/468 | Cost: 0.2034 Epoch: 005/010 | Batch 000/468 | Cost: 0.2358 Epoch: 005/010 | Batch 200/468 | Cost: 0.2009 Epoch: 005/010 | Batch 400/468 | Cost: 0.0824 Epoch: 006/010 | Batch 000/468 | Cost: 0.1473 Epoch: 006/010 | Batch 200/468 | Cost: 0.1436 Epoch: 006/010 | Batch 400/468 | Cost: 0.2141 Epoch: 007/010 | Batch 000/468 | Cost: 0.3440 Epoch: 007/010 | Batch 200/468 | Cost: 0.1331 Epoch: 007/010 | Batch 400/468 | Cost: 0.1123 Epoch: 008/010 | Batch 000/468 | Cost: 0.1986 Epoch: 008/010 | Batch 200/468 | Cost: 0.1621 Epoch: 008/010 | Batch 400/468 | Cost: 0.1798 Epoch: 009/010 | Batch 000/468 | Cost: 0.1393 Epoch: 009/010 | Batch 200/468 | Cost: 0.2675 Epoch: 009/010 | Batch 400/468 | Cost: 0.1482 Epoch: 010/010 | Batch 000/468 | Cost: 0.1138 Epoch: 010/010 | Batch 200/468 | Cost: 0.1673 Epoch: 010/010 | Batch 400/468 | Cost: 0.1629","title":"3 - Using the CONDOR loss for model training"},{"location":"tutorials/mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the logits_to_label utility function from condor_pytorch to convert the probabilities back to the orginal label. from condor_pytorch.dataset import logits_to_label from condor_pytorch.activations import ordinal_softmax from condor_pytorch.metrics import earth_movers_distance from condor_pytorch.metrics import ordinal_accuracy from condor_pytorch.metrics import mean_absolute_error def compute_mae_and_acc(model, data_loader, device): with torch.no_grad(): emd, mae, acc, acc1, num_examples = 0., 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(targets, num_classes=NUM_CLASSES) features = features.to(device) levels = levels.to(device) targets = targets.float().to(device) ids = targets.long() logits = model(features) predicted_labels = logits_to_label(logits).float() predicted_probs = ordinal_softmax(logits).float() num_examples += targets.size(0) mae += mean_absolute_error(logits,levels,reduction='sum') acc += ordinal_accuracy(logits,levels,tolerance=0,reduction='sum') acc1 += ordinal_accuracy(logits,levels,tolerance=1,reduction='sum') emd += earth_movers_distance(logits,levels,reduction='sum') mae = mae / num_examples acc = acc / num_examples acc1 = acc1 / num_examples emd = emd / num_examples return mae, acc, acc1, emd train_mae, train_acc, train_acc1, train_emd = compute_mae_and_acc(model, train_loader, DEVICE) test_mae, test_acc, test_acc1, test_emd = compute_mae_and_acc(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Accuracy tolerance 0 (train/test): {train_acc:.2f} | {test_acc:.2f}') print(f'Accuracy tolerance 1 (train/test): {train_acc1:.2f} | {test_acc1:.2f}') print(f'Earth movers distance (train/test): {train_emd:.3f} | {test_emd:.3f}') Mean absolute error (train/test): 0.15 | 0.15 Accuracy tolerance 0 (train/test): 0.96 | 0.96 Accuracy tolerance 1 (train/test): 0.97 | 0.97 Earth movers distance (train/test): 0.251 | 0.247","title":"4 -- Evaluate model"},{"location":"tutorials/poker/","text":"CONDOR MLP for predicting poker hands This tutorial explains how to equip a deep neural network with the CONDOR layer and loss function for ordinal regression in the context of predicting poker hands. 0 -- Obtaining and preparing the Poker Hand dataset from the UCI ML repository First, we are going to download and prepare the UCI Poker Hand dataset from https://archive.ics.uci.edu/ml/datasets/Poker+Hand and save it as CSV files locally. This is a general procedure that is not specific to CONDOR. This dataset has 10 ordinal labels, 0: Nothing in hand; not a recognized poker hand 1: One pair; one pair of equal ranks within five cards 2: Two pairs; two pairs of equal ranks within five cards 3: Three of a kind; three equal ranks within five cards 4: Straight; five cards, sequentially ranked with no gaps 5: Flush; five cards with the same suit 6: Full house; pair + different rank three of a kind 7: Four of a kind; four equal ranks within five cards 8: Straight flush; straight + flush 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush where 0 < 1 < 2 ... < 9. Download training examples and test dataset: import pandas as pd train_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\", header=None) train_features = train_df.loc[:, 0:10] train_labels = train_df.loc[:, 10] print('Number of features:', train_features.shape[1]) print('Number of training examples:', train_features.shape[0]) Number of features: 11 Number of training examples: 25010 test_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\", header=None) test_df.head() test_features = test_df.loc[:, 0:10] test_labels = test_df.loc[:, 10] print('Number of test examples:', test_features.shape[0]) Number of test examples: 1000000 Standardize features: from sklearn.preprocessing import StandardScaler sc = StandardScaler() train_features_sc = sc.fit_transform(train_features) test_features_sc = sc.transform(test_features) Save training and test set as CSV files locally pd.DataFrame(train_features_sc).to_csv('train_features.csv', index=False) train_labels.to_csv('train_labels.csv', index=False) pd.DataFrame(test_features_sc).to_csv('test_features.csv', index=False) test_labels.to_csv('test_labels.csv', index=False) # don't need those anymore del test_features del train_features del train_labels del test_labels 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CONDOR. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) Training on cpu from torch.utils.data import Dataset import numpy as np class MyDataset(Dataset): def __init__(self, csv_path_features, csv_path_labels, dtype=np.float32): self.features = pd.read_csv(csv_path_features).values.astype(np.float32) self.labels = pd.read_csv(csv_path_labels).values.flatten() def __getitem__(self, index): inputs = self.features[index] label = self.labels[index] return inputs, label def __len__(self): return self.labels.shape[0] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset('train_features.csv', 'train_labels.csv') test_dataset = MyDataset('test_features.csv', 'test_labels.csv') train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use # Checking the dataset for inputs, labels in train_loader: print('Input batch dimensions:', inputs.shape) print('Input label dimensions:', labels.shape) break Input batch dimensions: torch.Size([128, 11]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with CONDOR layer In this section, we are using condor_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CONDOR method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CONDOR layer with one fewer node. class CondorMLP(torch.nn.Module): def __init__(self, num_classes): super(CondorMLP, self).__init__() self.features = torch.nn.Sequential( torch.nn.Linear(11, 5), torch.nn.ReLU(), torch.nn.Linear(5, 5), torch.nn.ReLU(), torch.nn.Linear(5,num_classes-1) #THIS IS KEY OUTPUT SIZE ) def forward(self, x): logits = self.features(x) return logits torch.manual_seed(random_seed) model = CondorMLP(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters()) 3 - Using the CONDOR loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via condor_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CONDOR loss (also provided via condor_pytorch ): cost = condor_negloglikeloss(logits, levels) from condor_pytorch.dataset import levels_from_labelbatch from condor_pytorch.losses import condor_negloglikeloss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits = model(features) #### CONDOR loss cost = cost = condor_negloglikeloss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) Epoch: 001/020 | Batch 000/196 | Cost: 1.1676 Epoch: 002/020 | Batch 000/196 | Cost: 1.1595 Epoch: 003/020 | Batch 000/196 | Cost: 0.5821 Epoch: 004/020 | Batch 000/196 | Cost: 0.2216 Epoch: 005/020 | Batch 000/196 | Cost: 0.1225 Epoch: 006/020 | Batch 000/196 | Cost: 0.1025 Epoch: 007/020 | Batch 000/196 | Cost: 0.0678 Epoch: 008/020 | Batch 000/196 | Cost: 0.0501 Epoch: 009/020 | Batch 000/196 | Cost: 0.0422 Epoch: 010/020 | Batch 000/196 | Cost: 0.0219 Epoch: 011/020 | Batch 000/196 | Cost: 0.0082 Epoch: 012/020 | Batch 000/196 | Cost: 0.0158 Epoch: 013/020 | Batch 000/196 | Cost: 0.0144 Epoch: 014/020 | Batch 000/196 | Cost: 0.0032 Epoch: 015/020 | Batch 000/196 | Cost: 0.0238 Epoch: 016/020 | Batch 000/196 | Cost: 0.0434 Epoch: 017/020 | Batch 000/196 | Cost: 0.0112 Epoch: 018/020 | Batch 000/196 | Cost: 0.0018 Epoch: 019/020 | Batch 000/196 | Cost: 0.0230 Epoch: 020/020 | Batch 000/196 | Cost: 0.0011 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the logits_to_label utility function from condor_pytorch to convert the probabilities back to the orginal label. from condor_pytorch.dataset import logits_to_label from condor_pytorch.activations import ordinal_softmax from condor_pytorch.metrics import earth_movers_distance from condor_pytorch.metrics import ordinal_accuracy from condor_pytorch.metrics import mean_absolute_error def compute_mae_and_acc(model, data_loader, device): with torch.no_grad(): emd, mae, acc, acc1, num_examples = 0., 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(targets, num_classes=NUM_CLASSES) features = features.to(device) levels = levels.to(device) targets = targets.float().to(device) ids = targets.long() logits = model(features) predicted_labels = logits_to_label(logits).float() predicted_probs = ordinal_softmax(logits).float() num_examples += targets.size(0) mae += mean_absolute_error(logits,levels,reduction='sum') acc += ordinal_accuracy(logits,levels,tolerance=0,reduction='sum') acc1 += ordinal_accuracy(logits,levels,tolerance=1,reduction='sum') emd += earth_movers_distance(logits,levels,reduction='sum') mae = mae / num_examples acc = acc / num_examples acc1 = acc1 / num_examples emd = emd / num_examples return mae, acc, acc1, emd train_mae, train_acc, train_acc1, train_emd = compute_mae_and_acc(model, train_loader, DEVICE) test_mae, test_acc, test_acc1, test_emd = compute_mae_and_acc(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Accuracy tolerance 0 (train/test): {train_acc:.2f} | {test_acc:.2f}') print(f'Accuracy tolerance 1 (train/test): {train_acc1:.2f} | {test_acc1:.2f}') print(f'Earth movers distance (train/test): {train_emd:.3f} | {test_emd:.3f}') Mean absolute error (train/test): 0.00 | 0.00 Accuracy tolerance 0 (train/test): 1.00 | 1.00 Accuracy tolerance 1 (train/test): 1.00 | 1.00 Earth movers distance (train/test): 0.005 | 0.004","title":"Poker Hands"},{"location":"tutorials/poker/#condor-mlp-for-predicting-poker-hands","text":"This tutorial explains how to equip a deep neural network with the CONDOR layer and loss function for ordinal regression in the context of predicting poker hands.","title":"CONDOR MLP for predicting poker hands"},{"location":"tutorials/poker/#0-obtaining-and-preparing-the-poker-hand-dataset-from-the-uci-ml-repository","text":"First, we are going to download and prepare the UCI Poker Hand dataset from https://archive.ics.uci.edu/ml/datasets/Poker+Hand and save it as CSV files locally. This is a general procedure that is not specific to CONDOR. This dataset has 10 ordinal labels, 0: Nothing in hand; not a recognized poker hand 1: One pair; one pair of equal ranks within five cards 2: Two pairs; two pairs of equal ranks within five cards 3: Three of a kind; three equal ranks within five cards 4: Straight; five cards, sequentially ranked with no gaps 5: Flush; five cards with the same suit 6: Full house; pair + different rank three of a kind 7: Four of a kind; four equal ranks within five cards 8: Straight flush; straight + flush 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush where 0 < 1 < 2 ... < 9. Download training examples and test dataset: import pandas as pd train_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\", header=None) train_features = train_df.loc[:, 0:10] train_labels = train_df.loc[:, 10] print('Number of features:', train_features.shape[1]) print('Number of training examples:', train_features.shape[0]) Number of features: 11 Number of training examples: 25010 test_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\", header=None) test_df.head() test_features = test_df.loc[:, 0:10] test_labels = test_df.loc[:, 10] print('Number of test examples:', test_features.shape[0]) Number of test examples: 1000000 Standardize features: from sklearn.preprocessing import StandardScaler sc = StandardScaler() train_features_sc = sc.fit_transform(train_features) test_features_sc = sc.transform(test_features) Save training and test set as CSV files locally pd.DataFrame(train_features_sc).to_csv('train_features.csv', index=False) train_labels.to_csv('train_labels.csv', index=False) pd.DataFrame(test_features_sc).to_csv('test_features.csv', index=False) test_labels.to_csv('test_labels.csv', index=False) # don't need those anymore del test_features del train_features del train_labels del test_labels","title":"0 -- Obtaining and preparing the Poker Hand dataset from the UCI ML repository"},{"location":"tutorials/poker/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CONDOR. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) Training on cpu from torch.utils.data import Dataset import numpy as np class MyDataset(Dataset): def __init__(self, csv_path_features, csv_path_labels, dtype=np.float32): self.features = pd.read_csv(csv_path_features).values.astype(np.float32) self.labels = pd.read_csv(csv_path_labels).values.flatten() def __getitem__(self, index): inputs = self.features[index] label = self.labels[index] return inputs, label def __len__(self): return self.labels.shape[0] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset('train_features.csv', 'train_labels.csv') test_dataset = MyDataset('test_features.csv', 'test_labels.csv') train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use # Checking the dataset for inputs, labels in train_loader: print('Input batch dimensions:', inputs.shape) print('Input label dimensions:', labels.shape) break Input batch dimensions: torch.Size([128, 11]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/poker/#2-equipping-mlp-with-condor-layer","text":"In this section, we are using condor_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CONDOR method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CONDOR layer with one fewer node. class CondorMLP(torch.nn.Module): def __init__(self, num_classes): super(CondorMLP, self).__init__() self.features = torch.nn.Sequential( torch.nn.Linear(11, 5), torch.nn.ReLU(), torch.nn.Linear(5, 5), torch.nn.ReLU(), torch.nn.Linear(5,num_classes-1) #THIS IS KEY OUTPUT SIZE ) def forward(self, x): logits = self.features(x) return logits torch.manual_seed(random_seed) model = CondorMLP(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters())","title":"2 - Equipping MLP with CONDOR layer"},{"location":"tutorials/poker/#3-using-the-condor-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via condor_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CONDOR loss (also provided via condor_pytorch ): cost = condor_negloglikeloss(logits, levels) from condor_pytorch.dataset import levels_from_labelbatch from condor_pytorch.losses import condor_negloglikeloss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits = model(features) #### CONDOR loss cost = cost = condor_negloglikeloss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) Epoch: 001/020 | Batch 000/196 | Cost: 1.1676 Epoch: 002/020 | Batch 000/196 | Cost: 1.1595 Epoch: 003/020 | Batch 000/196 | Cost: 0.5821 Epoch: 004/020 | Batch 000/196 | Cost: 0.2216 Epoch: 005/020 | Batch 000/196 | Cost: 0.1225 Epoch: 006/020 | Batch 000/196 | Cost: 0.1025 Epoch: 007/020 | Batch 000/196 | Cost: 0.0678 Epoch: 008/020 | Batch 000/196 | Cost: 0.0501 Epoch: 009/020 | Batch 000/196 | Cost: 0.0422 Epoch: 010/020 | Batch 000/196 | Cost: 0.0219 Epoch: 011/020 | Batch 000/196 | Cost: 0.0082 Epoch: 012/020 | Batch 000/196 | Cost: 0.0158 Epoch: 013/020 | Batch 000/196 | Cost: 0.0144 Epoch: 014/020 | Batch 000/196 | Cost: 0.0032 Epoch: 015/020 | Batch 000/196 | Cost: 0.0238 Epoch: 016/020 | Batch 000/196 | Cost: 0.0434 Epoch: 017/020 | Batch 000/196 | Cost: 0.0112 Epoch: 018/020 | Batch 000/196 | Cost: 0.0018 Epoch: 019/020 | Batch 000/196 | Cost: 0.0230 Epoch: 020/020 | Batch 000/196 | Cost: 0.0011","title":"3 - Using the CONDOR loss for model training"},{"location":"tutorials/poker/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the logits_to_label utility function from condor_pytorch to convert the probabilities back to the orginal label. from condor_pytorch.dataset import logits_to_label from condor_pytorch.activations import ordinal_softmax from condor_pytorch.metrics import earth_movers_distance from condor_pytorch.metrics import ordinal_accuracy from condor_pytorch.metrics import mean_absolute_error def compute_mae_and_acc(model, data_loader, device): with torch.no_grad(): emd, mae, acc, acc1, num_examples = 0., 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): ##### Convert class labels for CONDOR levels = levels_from_labelbatch(targets, num_classes=NUM_CLASSES) features = features.to(device) levels = levels.to(device) targets = targets.float().to(device) ids = targets.long() logits = model(features) predicted_labels = logits_to_label(logits).float() predicted_probs = ordinal_softmax(logits).float() num_examples += targets.size(0) mae += mean_absolute_error(logits,levels,reduction='sum') acc += ordinal_accuracy(logits,levels,tolerance=0,reduction='sum') acc1 += ordinal_accuracy(logits,levels,tolerance=1,reduction='sum') emd += earth_movers_distance(logits,levels,reduction='sum') mae = mae / num_examples acc = acc / num_examples acc1 = acc1 / num_examples emd = emd / num_examples return mae, acc, acc1, emd train_mae, train_acc, train_acc1, train_emd = compute_mae_and_acc(model, train_loader, DEVICE) test_mae, test_acc, test_acc1, test_emd = compute_mae_and_acc(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Accuracy tolerance 0 (train/test): {train_acc:.2f} | {test_acc:.2f}') print(f'Accuracy tolerance 1 (train/test): {train_acc1:.2f} | {test_acc1:.2f}') print(f'Earth movers distance (train/test): {train_emd:.3f} | {test_emd:.3f}') Mean absolute error (train/test): 0.00 | 0.00 Accuracy tolerance 0 (train/test): 1.00 | 1.00 Accuracy tolerance 1 (train/test): 1.00 | 1.00 Earth movers distance (train/test): 0.005 | 0.004","title":"4 -- Evaluate model"}]}