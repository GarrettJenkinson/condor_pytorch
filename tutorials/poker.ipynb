{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONDOR MLP for predicting poker hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial explains how to equip a deep neural network with the CONDOR layer and loss function for ordinal regression in the context of predicting poker hands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 -- Obtaining and preparing the Poker Hand dataset from the UCI ML repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to download and prepare the UCI Poker Hand dataset from [https://archive.ics.uci.edu/ml/datasets/Poker+Hand](https://archive.ics.uci.edu/ml/datasets/Poker+Hand) and save it as CSV files locally. This is a general procedure that is not specific to CONDOR.\n",
    "\n",
    "This dataset has 10 ordinal labels, \n",
    "\n",
    "```\n",
    "0: Nothing in hand; not a recognized poker hand \n",
    "1: One pair; one pair of equal ranks within five cards \n",
    "2: Two pairs; two pairs of equal ranks within five cards \n",
    "3: Three of a kind; three equal ranks within five cards \n",
    "4: Straight; five cards, sequentially ranked with no gaps \n",
    "5: Flush; five cards with the same suit \n",
    "6: Full house; pair + different rank three of a kind \n",
    "7: Four of a kind; four equal ranks within five cards \n",
    "8: Straight flush; straight + flush \n",
    "9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush \n",
    "```\n",
    "\n",
    "where 0 < 1 < 2 ... < 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download training examples and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 11\n",
      "Number of training examples: 25010\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\", header=None)\n",
    "train_features = train_df.loc[:, 0:10]\n",
    "train_labels = train_df.loc[:, 10]\n",
    "\n",
    "print('Number of features:', train_features.shape[1])\n",
    "print('Number of training examples:', train_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: 1000000\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\", header=None)\n",
    "test_df.head()\n",
    "\n",
    "test_features = test_df.loc[:, 0:10]\n",
    "test_labels = test_df.loc[:, 10]\n",
    "\n",
    "print('Number of test examples:', test_features.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_features_sc = sc.fit_transform(train_features)\n",
    "test_features_sc = sc.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training and test set as CSV files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_features_sc).to_csv('train_features.csv', index=False)\n",
    "train_labels.to_csv('train_labels.csv', index=False)\n",
    "\n",
    "pd.DataFrame(test_features_sc).to_csv('test_features.csv', index=False)\n",
    "test_labels.to_csv('test_labels.csv', index=False)\n",
    "\n",
    "# don't need those anymore\n",
    "del test_features\n",
    "del train_features\n",
    "del train_labels\n",
    "del test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -- Setting up the dataset and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CONDOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_path_features, csv_path_labels, dtype=np.float32):\n",
    "    \n",
    "        self.features = pd.read_csv(csv_path_features).values.astype(np.float32)\n",
    "        self.labels = pd.read_csv(csv_path_labels).values.flatten()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return inputs, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([128, 11])\n",
      "Input label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = MyDataset('train_features.csv', 'train_labels.csv')\n",
    "test_dataset = MyDataset('test_features.csv', 'test_labels.csv')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # want to shuffle the dataset\n",
    "                          num_workers=0) # number processes/CPUs to use\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True, # want to shuffle the dataset\n",
    "                         num_workers=0) # number processes/CPUs to use\n",
    "\n",
    "# Checking the dataset\n",
    "for inputs, labels in train_loader:  \n",
    "    print('Input batch dimensions:', inputs.shape)\n",
    "    print('Input label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Equipping MLP with CONDOR layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are using  `condor_pytorch` to outfit a multilayer perceptron for ordinal regression. Note that the CONDOR method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CONDOR layer with one fewer node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondorMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(CondorMLP, self).__init__()\n",
    "\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Linear(11, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(5, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(5,num_classes-1) #THIS IS KEY OUTPUT SIZE\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = CondorMLP(num_classes=NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Using the CONDOR loss for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, all you need to do is to \n",
    "\n",
    "1) convert the integer class labels into the extended binary label format using the `levels_from_labelbatch` provided via `condor_pytorch`:\n",
    "\n",
    "```python\n",
    "        levels = levels_from_labelbatch(class_labels, \n",
    "                                        num_classes=NUM_CLASSES)\n",
    "```\n",
    "\n",
    "2) Apply the CONDOR loss (also provided via `condor_pytorch`):\n",
    "\n",
    "```python\n",
    "        cost = condor_negloglikeloss(logits, levels)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Batch 000/196 | Cost: 1.1676\n",
      "Epoch: 002/020 | Batch 000/196 | Cost: 1.1595\n",
      "Epoch: 003/020 | Batch 000/196 | Cost: 0.5821\n",
      "Epoch: 004/020 | Batch 000/196 | Cost: 0.2216\n",
      "Epoch: 005/020 | Batch 000/196 | Cost: 0.1225\n",
      "Epoch: 006/020 | Batch 000/196 | Cost: 0.1025\n",
      "Epoch: 007/020 | Batch 000/196 | Cost: 0.0678\n",
      "Epoch: 008/020 | Batch 000/196 | Cost: 0.0501\n",
      "Epoch: 009/020 | Batch 000/196 | Cost: 0.0422\n",
      "Epoch: 010/020 | Batch 000/196 | Cost: 0.0219\n",
      "Epoch: 011/020 | Batch 000/196 | Cost: 0.0082\n",
      "Epoch: 012/020 | Batch 000/196 | Cost: 0.0158\n",
      "Epoch: 013/020 | Batch 000/196 | Cost: 0.0144\n",
      "Epoch: 014/020 | Batch 000/196 | Cost: 0.0032\n",
      "Epoch: 015/020 | Batch 000/196 | Cost: 0.0238\n",
      "Epoch: 016/020 | Batch 000/196 | Cost: 0.0434\n",
      "Epoch: 017/020 | Batch 000/196 | Cost: 0.0112\n",
      "Epoch: 018/020 | Batch 000/196 | Cost: 0.0018\n",
      "Epoch: 019/020 | Batch 000/196 | Cost: 0.0230\n",
      "Epoch: 020/020 | Batch 000/196 | Cost: 0.0011\n"
     ]
    }
   ],
   "source": [
    "from condor_pytorch.dataset import levels_from_labelbatch\n",
    "from condor_pytorch.losses import condor_negloglikeloss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        ##### Convert class labels for CONDOR\n",
    "        levels = levels_from_labelbatch(class_labels, \n",
    "                                        num_classes=NUM_CLASSES)\n",
    "        ###--------------------------------------------------------------------###\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        levels = levels.to(DEVICE)\n",
    "        logits = model(features)\n",
    "        \n",
    "        #### CONDOR loss \n",
    "        cost = cost = condor_negloglikeloss(logits, levels)\n",
    "        ###--------------------------------------------------------------------###   \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 200:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 -- Evaluate model\n",
    "\n",
    "Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures.\n",
    "\n",
    "For this, we are going to use the `logits_to_label` utility function from `condor_pytorch` to convert the probabilities back to the orginal label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from condor_pytorch.dataset import logits_to_label\n",
    "from condor_pytorch.activations import ordinal_softmax\n",
    "from condor_pytorch.metrics import earth_movers_distance\n",
    "from condor_pytorch.metrics import ordinal_accuracy\n",
    "from condor_pytorch.metrics import mean_absolute_error\n",
    "\n",
    "def compute_mae_and_acc(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        emd, mae, acc, acc1, num_examples = 0., 0., 0., 0., 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "            ##### Convert class labels for CONDOR\n",
    "            levels = levels_from_labelbatch(targets, \n",
    "                                        num_classes=NUM_CLASSES)\n",
    "            features = features.to(device)\n",
    "            levels = levels.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "            ids = targets.long()\n",
    "\n",
    "            logits = model(features)\n",
    "            predicted_labels = logits_to_label(logits).float()\n",
    "            predicted_probs = ordinal_softmax(logits).float()\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            mae  += mean_absolute_error(logits,levels,reduction='sum')\n",
    "            acc  += ordinal_accuracy(logits,levels,tolerance=0,reduction='sum')\n",
    "            acc1 += ordinal_accuracy(logits,levels,tolerance=1,reduction='sum')\n",
    "            emd  += earth_movers_distance(logits,levels,reduction='sum')\n",
    "\n",
    "        mae  = mae / num_examples\n",
    "        acc  = acc / num_examples\n",
    "        acc1 = acc1 / num_examples\n",
    "        emd  = emd / num_examples\n",
    "        return mae, acc, acc1, emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae, train_acc, train_acc1, train_emd = compute_mae_and_acc(model, train_loader, DEVICE)\n",
    "test_mae, test_acc, test_acc1, test_emd = compute_mae_and_acc(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error (train/test): 0.00 | 0.00\n",
      "Accuracy tolerance 0 (train/test): 1.00 | 1.00\n",
      "Accuracy tolerance 1 (train/test): 1.00 | 1.00\n",
      "Earth movers distance (train/test): 0.005 | 0.004\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}')\n",
    "print(f'Accuracy tolerance 0 (train/test): {train_acc:.2f} | {test_acc:.2f}')\n",
    "print(f'Accuracy tolerance 1 (train/test): {train_acc1:.2f} | {test_acc1:.2f}')\n",
    "print(f'Earth movers distance (train/test): {train_emd:.3f} | {test_emd:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
